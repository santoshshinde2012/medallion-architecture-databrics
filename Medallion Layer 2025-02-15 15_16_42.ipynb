{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eac1192-a7de-40df-a7f1-78835ba3c360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Using Medallion Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21aebdbc-c812-4c38-b3c5-e1ba6219eeed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- o_orderkey: long (nullable = true)\n |-- o_custkey: long (nullable = true)\n |-- o_orderstatus: string (nullable = true)\n |-- o_totalprice: decimal(18,2) (nullable = true)\n |-- o_orderdate: date (nullable = true)\n |-- o_orderpriority: string (nullable = true)\n |-- o_clerk: string (nullable = true)\n |-- o_shippriority: integer (nullable = true)\n |-- o_comment: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"samples.tpch.orders\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c789bad-09ff-4f54-8fa8-9d7191ab7516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Layer Result (One Record):\nRow(o_orderkey=13710944, o_custkey=227285, o_orderstatus='O', o_totalprice=Decimal('162169.66'), o_orderdate=datetime.date(1995, 10, 11), o_orderpriority='1-URGENT', o_clerk='Clerk#000000432', o_shippriority=0, o_comment='accounts. ruthlessly regular accounts alongside of the car', ingest_time=datetime.datetime(2025, 2, 15, 11, 29, 25, 534388))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BronzeLayer\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.table(\"samples.tpch.orders\")\n",
    "\n",
    "# Create Bronze table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS retailsc_medallion.orders_bronze (\n",
    "        o_orderkey LONG,\n",
    "        o_custkey LONG,\n",
    "        o_orderstatus STRING,\n",
    "        o_totalprice DECIMAL(18,2),\n",
    "        o_orderdate DATE,\n",
    "        o_orderpriority STRING,\n",
    "        o_clerk STRING,\n",
    "        o_shippriority INTEGER,\n",
    "        o_comment STRING,\n",
    "        ingest_time TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Raw order data with ingestion timestamp.';\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into Bronze table\n",
    "bronze_orders = df.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "bronze_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retailsc_medallion.orders_bronze\")\n",
    "\n",
    "# Expected result for one record\n",
    "bronze_result = spark.sql(\"SELECT * FROM retailsc_medallion.orders_bronze LIMIT 1\").collect()[0]\n",
    "print(\"Bronze Layer Result (One Record):\")\n",
    "print(bronze_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b64333-08a9-4a42-8e3e-5bb16418e40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver Layer Result (One Record):\nRow(o_orderkey=11396166, o_custkey=179329, order_status='Open', o_totalprice=Decimal('187683.10'), o_orderdate=datetime.date(1996, 6, 22), o_orderpriority='4-NOT SPECIFIED', o_clerk='Clerk#000004689', o_shippriority=0, o_comment='ep fluffily regular packages. regular, final courts ag', ingest_time=datetime.datetime(2025, 2, 15, 11, 29, 25, 534388), o_orderstatus='O')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SilverLayer\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Silver table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS retailsc_medallion.orders_silver (\n",
    "        o_orderkey LONG,\n",
    "        o_custkey LONG,\n",
    "        order_status STRING,\n",
    "        o_totalprice DECIMAL(18,2),\n",
    "        o_orderdate DATE,\n",
    "        o_orderpriority STRING,\n",
    "        o_clerk STRING,\n",
    "        o_shippriority INTEGER,\n",
    "        o_comment STRING,\n",
    "        ingest_time TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Cleaned and normalized order data.';\n",
    "\"\"\")\n",
    "\n",
    "# Clean and transform data for Silver layer\n",
    "silver_orders = spark.table(\"retailsc_medallion.orders_bronze\") \\\n",
    "    .filter(F.col(\"o_totalprice\") > 0) \\\n",
    "    .withColumn(\"order_status\", \n",
    "        F.when(F.col(\"o_orderstatus\") == \"F\", \"Fulfilled\")\n",
    "         .when(F.col(\"o_orderstatus\") == \"O\", \"Open\")\n",
    "         .otherwise(\"Unknown\")\n",
    "    )\n",
    "\n",
    "# Insert data into Silver table\n",
    "silver_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retailsc_medallion.orders_silver\")\n",
    "\n",
    "# Expected result for one record\n",
    "silver_result = spark.sql(\"SELECT * FROM retailsc_medallion.orders_silver LIMIT 1\").collect()[0]\n",
    "print(\"Silver Layer Result (One Record):\")\n",
    "print(silver_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a788c77d-8fb1-4a94-91bd-91447be89247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copper Layer Result (One Record):\nRow(order_key=5611649, customer_key=687736, order_status='Open', total_price=Decimal('51905.72'), order_date=datetime.date(1996, 6, 4), order_priority='5-LOW', clerk='Clerk#000002954', ship_priority=0, comment='ls! bold, regular accounts ', ingest_time=datetime.datetime(2025, 2, 15, 11, 29, 25, 534388), order_year=1996, order_month=6, order_day=3, order_value_segment='High', o_orderstatus='O')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CopperLayer\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Copper table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS retailsc_medallion.orders_copper (\n",
    "        order_key LONG,\n",
    "        customer_key LONG,\n",
    "        order_status STRING,\n",
    "        total_price DECIMAL(18,2),\n",
    "        order_date DATE,\n",
    "        order_priority STRING,\n",
    "        clerk STRING,\n",
    "        ship_priority INTEGER,\n",
    "        comment STRING,\n",
    "        ingest_time TIMESTAMP,\n",
    "        order_year INTEGER,\n",
    "        order_month INTEGER,\n",
    "        order_day INTEGER,\n",
    "        order_value_segment STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Advanced transformations including derived fields for time-based analysis and segmentation.';\n",
    "\"\"\")\n",
    "\n",
    "# Transform data for Copper layer\n",
    "copper_orders = spark.table(\"retailsc_medallion.orders_silver\") \\\n",
    "    .withColumnRenamed(\"o_orderkey\", \"order_key\") \\\n",
    "    .withColumnRenamed(\"o_custkey\", \"customer_key\") \\\n",
    "    .withColumnRenamed(\"o_totalprice\", \"total_price\") \\\n",
    "    .withColumnRenamed(\"o_orderdate\", \"order_date\") \\\n",
    "    .withColumnRenamed(\"o_orderpriority\", \"order_priority\") \\\n",
    "    .withColumnRenamed(\"o_clerk\", \"clerk\") \\\n",
    "    .withColumnRenamed(\"o_shippriority\", \"ship_priority\") \\\n",
    "    .withColumnRenamed(\"o_comment\", \"comment\") \\\n",
    "    .withColumn(\"order_year\", F.year(\"order_date\")) \\\n",
    "    .withColumn(\"order_month\", F.month(\"order_date\")) \\\n",
    "    .withColumn(\"order_day\", F.dayofweek(\"order_date\")) \\\n",
    "    .withColumn(\"order_value_segment\", \n",
    "        F.when(F.col(\"total_price\") > 500, \"High\")\n",
    "         .when(F.col(\"total_price\") > 100, \"Medium\")\n",
    "         .otherwise(\"Low\")\n",
    "    )\n",
    "\n",
    "# Insert data into Copper table\n",
    "copper_orders.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retailsc_medallion.orders_copper\")\n",
    "\n",
    "# Expected result for one record\n",
    "copper_result = spark.sql(\"SELECT * FROM retailsc_medallion.orders_copper LIMIT 1\").collect()[0]\n",
    "print(\"Copper Layer Result (One Record):\")\n",
    "print(copper_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f909c6e4-86be-4881-857d-24aa0c423f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Layer Daily Sales Result (One Record):\nRow(order_year=1995, order_month=12, order_day=4, total_sales=Decimal('1880689027.98'), order_count=12450)\nGold Layer Customer Segmentation Result (One Record):\nRow(customer_key=183637, order_year=1994, order_month=1, order_value_segment='High', orders_per_segment=1, avg_order_value=Decimal('163460.83'))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GoldLayer\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Daily Sales Aggregation\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS retailsc_medallion.orders_gold_daily_sales (\n",
    "        order_year INTEGER,\n",
    "        order_month INTEGER,\n",
    "        order_day INTEGER,\n",
    "        total_sales DECIMAL(18,2),\n",
    "        order_count BIGINT\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Aggregates daily sales for BI reporting.';\n",
    "\"\"\")\n",
    "\n",
    "gold_daily_sales = spark.table(\"retailsc_medallion.orders_copper\").groupBy(\"order_year\", \"order_month\", \"order_day\") \\\n",
    "    .agg(\n",
    "        F.sum(\"total_price\").cast(\"decimal(18,2)\").alias(\"total_sales\"),\n",
    "        F.count(\"order_key\").cast(\"bigint\").alias(\"order_count\")\n",
    "    )\n",
    "\n",
    "gold_daily_sales.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retailsc_medallion.orders_gold_daily_sales\")\n",
    "\n",
    "# Customer Segmentation\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS retailsc_medallion.orders_gold_customer_segment (\n",
    "        customer_key LONG,\n",
    "        order_year INTEGER,\n",
    "        order_month INTEGER,\n",
    "        order_value_segment STRING,\n",
    "        orders_per_segment BIGINT,\n",
    "        avg_order_value DECIMAL(18,2)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Aggregates customer orders for segmentation analysis.';\n",
    "\"\"\")\n",
    "\n",
    "gold_customer_segment = spark.table(\"retailsc_medallion.orders_copper\").groupBy(\"customer_key\", \"order_year\", \"order_month\", \"order_value_segment\") \\\n",
    "    .agg(\n",
    "        F.count(\"order_key\").cast(\"bigint\").alias(\"orders_per_segment\"),\n",
    "        F.avg(\"total_price\").cast(\"decimal(18,2)\").alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "gold_customer_segment.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retailsc_medallion.orders_gold_customer_segment\")\n",
    "\n",
    "# Expected results for one record each\n",
    "gold_daily_result = spark.sql(\"SELECT * FROM retailsc_medallion.orders_gold_daily_sales LIMIT 1\").collect()[0]\n",
    "gold_customer_result = spark.sql(\"SELECT * FROM retailsc_medallion.orders_gold_customer_segment LIMIT 1\").collect()[0]\n",
    "\n",
    "print(\"Gold Layer Daily Sales Result (One Record):\")\n",
    "print(gold_daily_result)\n",
    "print(\"Gold Layer Customer Segmentation Result (One Record):\")\n",
    "print(gold_customer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c776f94-d3a5-49b7-ae02-2f6cc05d2076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platinum Layer Customer Insights Result (One Record):\nRow(customer_key=183637, order_year=1994, order_month=1, order_value_segment='High', orders_per_segment=1, avg_order_value=Decimal('163460.83'), total_purchase_value=Decimal('163460.83'))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PlatinumLayer\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# If the focus is on customer behavior or order attributes without product specifics:\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS retailsc_medallion.orders_platinum_customer_insights (\n",
    "        customer_key LONG,\n",
    "        order_year INTEGER,\n",
    "        order_month INTEGER,\n",
    "        order_value_segment STRING,\n",
    "        orders_per_segment BIGINT,\n",
    "        avg_order_value DECIMAL(18,2),\n",
    "        total_purchase_value DECIMAL(18,2)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Specialized dataset for customer insights, focusing on behavior and purchase patterns.';\n",
    "\"\"\")\n",
    "\n",
    "# Explicitly cast the total_purchase_value to ensure type consistency\n",
    "platinum_customer_insights = spark.table(\"retailsc_medallion.orders_gold_customer_segment\") \\\n",
    "    .withColumn(\"total_purchase_value\", (F.col(\"avg_order_value\") * F.col(\"orders_per_segment\")).cast(\"decimal(18,2)\"))\n",
    "\n",
    "# Ensure all columns match the schema before writing\n",
    "platinum_customer_insights = platinum_customer_insights.select(\n",
    "    F.col(\"customer_key\"),\n",
    "    F.col(\"order_year\"),\n",
    "    F.col(\"order_month\"),\n",
    "    F.col(\"order_value_segment\"),\n",
    "    F.col(\"orders_per_segment\").cast(\"bigint\"),\n",
    "    F.col(\"avg_order_value\").cast(\"decimal(18,2)\"),\n",
    "    F.col(\"total_purchase_value\").cast(\"decimal(18,2)\")\n",
    ")\n",
    "\n",
    "platinum_customer_insights.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"retailsc_medallion.orders_platinum_customer_insights\")\n",
    "\n",
    "# Expected result for one record\n",
    "platinum_result = spark.sql(\"SELECT * FROM retailsc_medallion.orders_platinum_customer_insights LIMIT 1\").collect()[0]\n",
    "print(\"Platinum Layer Customer Insights Result (One Record):\")\n",
    "print(platinum_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Medallion Layer 2025-02-15 15:16:42",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}